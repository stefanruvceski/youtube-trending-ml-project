{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&\n",
      "GB\n",
      "https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet&chart=mostPopular&regionCode=GB&maxResults=50&key=AIzaSyC4sS4U66SGjXFIQTsyXwj_Nf-LpODTxiU\n",
      "200\n",
      "&pageToken=CDIQAA&\n",
      "GB\n",
      "https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet&pageToken=CDIQAA&chart=mostPopular&regionCode=GB&maxResults=50&key=AIzaSyC4sS4U66SGjXFIQTsyXwj_Nf-LpODTxiU\n",
      "200\n",
      "&pageToken=CGQQAA&\n",
      "GB\n",
      "https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet&pageToken=CGQQAA&chart=mostPopular&regionCode=GB&maxResults=50&key=AIzaSyC4sS4U66SGjXFIQTsyXwj_Nf-LpODTxiU\n",
      "200\n",
      "&pageToken=CJYBEAA&\n",
      "GB\n",
      "https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet&pageToken=CJYBEAA&chart=mostPopular&regionCode=GB&maxResults=50&key=AIzaSyC4sS4U66SGjXFIQTsyXwj_Nf-LpODTxiU\n",
      "200\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/Comments_ahZFCF--uRY.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-037b0e6989e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mcountry_codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"country_codes.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountry_codes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcountry_codes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'END'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-037b0e6989e1>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcountry_code\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcountry_codes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mcountry_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mget_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountry_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[1;31m#print(country_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mwrite_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountry_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountry_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-037b0e6989e1>\u001b[0m in \u001b[0;36mget_pages\u001b[1;34m(country_code, next_page_token)\u001b[0m\n\u001b[0;32m    161\u001b[0m        \u001b[0mvideo_id\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m        \u001b[0mvideo_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m        \u001b[0mget_comments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m        \u001b[1;31m# print(\"usao\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;31m#print(next_page_token)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-037b0e6989e1>\u001b[0m in \u001b[0;36mget_comments\u001b[1;34m(video_id)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Comments_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".json\"\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_comments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/Comments_ahZFCF--uRY.json'"
     ]
    }
   ],
   "source": [
    "import requests, sys, time, os, argparse\n",
    "import json\n",
    "import numpy as np\n",
    "# List of simple to collect features\n",
    "snippet_features = [\"title\",\n",
    "                    \"publishedAt\",\n",
    "                    \"channelId\",\n",
    "                    \"channelTitle\",\n",
    "                    \"categoryId\"]\n",
    "\n",
    "# Any characters to exclude, generally these are things that become problematic in CSV files\n",
    "unsafe_characters = ['\\n', '\"']\n",
    "\n",
    "# Used to identify columns, currently hardcoded order\n",
    "header = [\"video_id\"] + snippet_features + [\"trending_date\", \"tags\", \"view_count\", \"likes\", \"dislikes\",\n",
    "                                            \"comment_count\", \"thumbnail_link\", \"comments_disabled\",\n",
    "                                            \"ratings_disabled\", \"description\"]\n",
    "\n",
    "\n",
    "def setup(api_path, code_path):\n",
    "    with open(api_path, 'r') as file:\n",
    "        api_key = file.readline()\n",
    "    \n",
    "    with open(code_path) as file:\n",
    "        country_codes = [x.rstrip() for x in file]\n",
    "    \n",
    "    return api_key, country_codes\n",
    "\n",
    "\n",
    "def prepare_feature(feature):\n",
    "    # Removes any character from the unsafe characters list and surrounds the whole item in quotes\n",
    "    for ch in unsafe_characters:\n",
    "        feature = str(feature).replace(ch, \"\")\n",
    "    return f'\"{feature}\"'\n",
    "\n",
    "\n",
    "def api_request(page_token, country_code):\n",
    "    # Builds the URL and requests the JSON from it\n",
    "    print(page_token)\n",
    "    print(country_code)\n",
    "    request_url = f\"https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet{page_token}chart=mostPopular&regionCode={country_code}&maxResults=50&key={api_key}\"\n",
    "    print(request_url)\n",
    "    request = requests.get(request_url)\n",
    "    print(request.status_code)\n",
    "    if request.status_code == 429:\n",
    "        print(\"Temp-Banned due to excess requests, please wait and continue later\")\n",
    "        sys.exit()\n",
    "    return request.json()\n",
    "def api_request_comments(page_token, video_id):\n",
    "    \n",
    "    request_comments_url = f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet{page_token}&videoId={video_id.strip()}&key=AIzaSyADlxGSg_VGSPtB6-rRkBw9g4eGSkdIUM4\"\n",
    "    #request_comments_url = \"https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet&chart=mostPopular&regionCode=US&maxResults=50&key=AIzaSyAT0_U4OzScfwyJheOesL9GZRzH5yA4JUY\"\n",
    "    #print(request_comments_url)\n",
    "    request = requests.get(request_comments_url)\n",
    "  \n",
    "    if request.status_code == 429:\n",
    "        print(\"Temp-Banned due to excess requests, please wait and continue later\")\n",
    "        sys.exit()\n",
    "    return request.json()\n",
    "\n",
    "def get_tags(tags_list):\n",
    "    # Takes a list of tags, prepares each tag and joins them into a string by the pipe character\n",
    "    return prepare_feature(\"|\".join(tags_list))\n",
    "\n",
    "\n",
    "def get_videos(items):\n",
    "    lines = []\n",
    "    for video in items:\n",
    "        comments_disabled = False\n",
    "        ratings_disabled = False\n",
    "\n",
    "        # We can assume something is wrong with the video if it has no statistics, often this means it has been deleted\n",
    "        # so we can just skip it\n",
    "        if \"statistics\" not in video:\n",
    "            continue\n",
    "\n",
    "        # A full explanation of all of these features can be found on the GitHub page for this project\n",
    "        video_id = prepare_feature(video['id'])\n",
    "\n",
    "        # Snippet and statistics are sub-dicts of video, containing the most useful info\n",
    "        snippet = video['snippet']\n",
    "        statistics = video['statistics']\n",
    "\n",
    "        # This list contains all of the features in snippet that are 1 deep and require no special processing\n",
    "        features = [prepare_feature(snippet.get(feature, \"\")) for feature in snippet_features]\n",
    "\n",
    "        # The following are special case features which require unique processing, or are not within the snippet dict\n",
    "        description = snippet.get(\"description\", \"\")\n",
    "        thumbnail_link = snippet.get(\"thumbnails\", dict()).get(\"default\", dict()).get(\"url\", \"\")\n",
    "        trending_date = time.strftime(\"%y.%d.%m\")\n",
    "        tags = get_tags(snippet.get(\"tags\", [\"[none]\"]))\n",
    "        view_count = statistics.get(\"viewCount\", 0)\n",
    "\n",
    "        # This may be unclear, essentially the way the API works is that if a video has comments or ratings disabled\n",
    "        # then it has no feature for it, thus if they don't exist in the statistics dict we know they are disabled\n",
    "        if 'likeCount' in statistics and 'dislikeCount' in statistics:\n",
    "            likes = statistics['likeCount']\n",
    "            dislikes = statistics['dislikeCount']\n",
    "        else:\n",
    "            ratings_disabled = True\n",
    "            likes = 0\n",
    "            dislikes = 0\n",
    "\n",
    "        if 'commentCount' in statistics:\n",
    "            comment_count = statistics['commentCount']\n",
    "        else:\n",
    "            comments_disabled = True\n",
    "            comment_count = 0\n",
    "\n",
    "        # Compiles all of the various bits of info into one consistently formatted line\n",
    "        line = [video_id] + features + [prepare_feature(x) for x in [trending_date, tags, view_count, likes, dislikes,\n",
    "                                                                       comment_count, thumbnail_link, comments_disabled,\n",
    "                                                                       ratings_disabled, description]]\n",
    "        lines.append(\",\".join(line))\n",
    "    return lines\n",
    "\n",
    "def get_comments(video_id):\n",
    "    video_comments = []\n",
    "    next_page_token = \"&\"\n",
    "    i =0;\n",
    "    while next_page_token is not None:\n",
    "        # A page of data i.e. a list of videos and all needed data\n",
    "        comments_data_page = api_request_comments(next_page_token, video_id)\n",
    "        i += 1\n",
    "        if(i == 10):\n",
    "            break;\n",
    "        \n",
    "        # Get the next page token and build a string which can be injected into the request with it, unless it's None,\n",
    "        # then let the whole thing be None so that the loop ends after this cycle\n",
    "        next_page_token = comments_data_page.get(\"nextPageToken\", None)\n",
    "        next_page_token = f\"&pageToken={next_page_token}&\" if next_page_token is not None else next_page_token\n",
    "\n",
    "        # Get all of the items as a list and let get_videos return the needed features\n",
    "        video_comments += comments_data_page.get('items', [])\n",
    "        \n",
    "    name = \"Comments_\"+video_id+\".json\";\n",
    "    with open(output_dir+'/'+name, 'w') as outfile:\n",
    "        json.dump(video_comments, outfile)\n",
    "    \n",
    "def get_pages(country_code, next_page_token=\"&\"):\n",
    "    country_data = []\n",
    "    i = 0\n",
    "    # Because the API uses page tokens (which are literally just the same function of numbers everywhere) it is much\n",
    "    # more inconvenient to iterate over pages, but that is what is done here.\n",
    "    while next_page_token is not None:\n",
    "        # A page of data i.e. a list of videos and all needed data\n",
    "        video_data_page = api_request(next_page_token, country_code)\n",
    "        \n",
    "        \n",
    "        # Get the next page token and build a string which can be injected into the request with it, unless it's None,\n",
    "        # then let the whole thing be None so that the loop ends after this cycle\n",
    "        next_page_token = video_data_page.get(\"nextPageToken\", None)\n",
    "        next_page_token = f\"&pageToken={next_page_token}&\" if next_page_token is not None else next_page_token\n",
    "\n",
    "        # Get all of the items as a list and let get_videos return the needed features\n",
    "        items = video_data_page.get('items', [])\n",
    "        country_data += get_videos(items)\n",
    "    #print(np.size(country_data))\n",
    "   # print(\"kod fora sam\")\n",
    "    for video in country_data:\n",
    "       video_id =  video.split(\",\")[0]\n",
    "       video_id = video_id.replace('\"', '')\n",
    "       get_comments(video_id)\n",
    "       # print(\"usao\")\n",
    "    #print(next_page_token)\n",
    "    return country_data\n",
    "\n",
    "\n",
    "def write_to_file(country_code, country_data):\n",
    "\n",
    "    print(f\"Writing {country_code} data to file...\")\n",
    "   \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    with open(f\"{output_dir}/{time.strftime('%y.%d.%m')}_{country_code}_videos.csv\", \"w+\", encoding='utf-8') as file:\n",
    "        for row in country_data:\n",
    "            file.write(f\"{row}\\n\")\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    for country_code in country_codes:\n",
    "     \n",
    "        country_data = [\",\".join(header)] + get_pages(country_code)\n",
    "        #print(country_data)\n",
    "        write_to_file(country_code, country_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    output_dir = \"output\"\n",
    "    api_key = \"api_key.txt\"\n",
    "    country_codes = \"country_codes.txt\"\n",
    "    api_key, country_codes= setup(api_key,country_codes)\n",
    "    get_data()\n",
    "    print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Stefan/api_key.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fff249bef2ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/Stefan/api_key.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[0mcountry_codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/Stefan/country_codes.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountry_codes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcountry_codes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m     \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-fff249bef2ba>\u001b[0m in \u001b[0;36msetup\u001b[1;34m(api_path, code_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Stefan/api_key.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests, sys, time, os, argparse\n",
    "\n",
    "# List of simple to collect features\n",
    "snippet_features = [\"title\",\n",
    "                    \"publishedAt\",\n",
    "                    \"channelId\",\n",
    "                    \"channelTitle\",\n",
    "                    \"categoryId\"]\n",
    "\n",
    "# Any characters to exclude, generally these are things that become problematic in CSV files\n",
    "unsafe_characters = ['\\n', '\"']\n",
    "\n",
    "# Used to identify columns, currently hardcoded order\n",
    "header = [\"video_id\"] + snippet_features + [\"trending_date\", \"tags\", \"view_count\", \"likes\", \"dislikes\",\n",
    "                                            \"comment_count\", \"thumbnail_link\", \"comments_disabled\",\n",
    "                                            \"ratings_disabled\", \"description\"]\n",
    "\n",
    "\n",
    "def setup(api_path, code_path):\n",
    "    with open(api_path, 'r') as file:\n",
    "        api_key = file.readline()\n",
    "\n",
    "    with open(code_path) as file:\n",
    "        country_codes = [x.rstrip() for x in file]\n",
    "\n",
    "    return api_key, country_codes\n",
    "\n",
    "\n",
    "def prepare_feature(feature):\n",
    "    # Removes any character from the unsafe characters list and surrounds the whole item in quotes\n",
    "    for ch in unsafe_characters:\n",
    "        feature = str(feature).replace(ch, \"\")\n",
    "    return f'\"{feature}\"'\n",
    "\n",
    "\n",
    "def api_request(page_token, country_code):\n",
    "    # Builds the URL and requests the JSON from it\n",
    "    request_url = f\"https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet{page_token}chart=mostPopular&regionCode={country_code}&maxResults=50&key={api_key}\"\n",
    "    display(request_url)\n",
    "    request = requests.get(request_url)\n",
    "    if request.status_code == 429:\n",
    "        print(\"Temp-Banned due to excess requests, please wait and continue later\")\n",
    "        sys.exit()\n",
    "    return request.json()\n",
    "\n",
    "\n",
    "def get_tags(tags_list):\n",
    "    # Takes a list of tags, prepares each tag and joins them into a string by the pipe character\n",
    "    return prepare_feature(\"|\".join(tags_list))\n",
    "\n",
    "\n",
    "def get_videos(items):\n",
    "    lines = []\n",
    "    for video in items:\n",
    "        comments_disabled = False\n",
    "        ratings_disabled = False\n",
    "\n",
    "        # We can assume something is wrong with the video if it has no statistics, often this means it has been deleted\n",
    "        # so we can just skip it\n",
    "        if \"statistics\" not in video:\n",
    "            continue\n",
    "\n",
    "        # A full explanation of all of these features can be found on the GitHub page for this project\n",
    "        video_id = prepare_feature(video['id'])\n",
    "\n",
    "        # Snippet and statistics are sub-dicts of video, containing the most useful info\n",
    "        snippet = video['snippet']\n",
    "        statistics = video['statistics']\n",
    "\n",
    "        # This list contains all of the features in snippet that are 1 deep and require no special processing\n",
    "        features = [prepare_feature(snippet.get(feature, \"\")) for feature in snippet_features]\n",
    "        \n",
    "        # The following are special case features which require unique processing, or are not within the snippet dict\n",
    "        description = snippet.get(\"description\", \"\")\n",
    "        thumbnail_link = snippet.get(\"thumbnails\", dict()).get(\"default\", dict()).get(\"url\", \"\")\n",
    "        trending_date = time.strftime(\"%y.%d.%m\")\n",
    "        tags = get_tags(snippet.get(\"tags\", [\"[none]\"]))\n",
    "        view_count = statistics.get(\"viewCount\", 0)\n",
    "\n",
    "        # This may be unclear, essentially the way the API works is that if a video has comments or ratings disabled\n",
    "        # then it has no feature for it, thus if they don't exist in the statistics dict we know they are disabled\n",
    "        if 'likeCount' in statistics and 'dislikeCount' in statistics:\n",
    "            likes = statistics['likeCount']\n",
    "            dislikes = statistics['dislikeCount']\n",
    "        else:\n",
    "            ratings_disabled = True\n",
    "            likes = 0\n",
    "            dislikes = 0\n",
    "\n",
    "        if 'commentCount' in statistics:\n",
    "            comment_count = statistics['commentCount']\n",
    "        else:\n",
    "            comments_disabled = True\n",
    "            comment_count = 0\n",
    "\n",
    "        # Compiles all of the various bits of info into one consistently formatted line\n",
    "        line = [video_id] + features + [prepare_feature(x) for x in [trending_date, tags, view_count, likes, dislikes,\n",
    "                                                                       comment_count, thumbnail_link, comments_disabled,\n",
    "                                                                       ratings_disabled, description]]\n",
    "        lines.append(\",\".join(line))\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_pages(country_code, next_page_token=\"&\"):\n",
    "    country_data = []\n",
    "\n",
    "    # Because the API uses page tokens (which are literally just the same function of numbers everywhere) it is much\n",
    "    # more inconvenient to iterate over pages, but that is what is done here.\n",
    "    while next_page_token is not None:\n",
    "        # A page of data i.e. a list of videos and all needed data\n",
    "        video_data_page = api_request(next_page_token, country_code)\n",
    "\n",
    "        # Get the next page token and build a string which can be injected into the request with it, unless it's None,\n",
    "        # then let the whole thing be None so that the loop ends after this cycle\n",
    "        next_page_token = video_data_page.get(\"nextPageToken\", None)\n",
    "        next_page_token = f\"&pageToken={next_page_token}&\" if next_page_token is not None else next_page_token\n",
    "\n",
    "        # Get all of the items as a list and let get_videos return the needed features\n",
    "        items = video_data_page.get('items', [])\n",
    "        country_data += get_videos(items)\n",
    "\n",
    "    return country_data\n",
    "\n",
    "\n",
    "def write_to_file(country_code, country_data):\n",
    "\n",
    "    print(f\"Writing {country_code} data to file...\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(f\"{output_dir}/{time.strftime('%y.%d.%m')}_{country_code}_videos.csv\", \"w+\", encoding='utf-8') as file:\n",
    "        for row in country_data:\n",
    "            file.write(f\"{row}\\n\")\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    for country_code in country_codes:\n",
    "        country_data = [\",\".join(header)] + get_pages(country_code)\n",
    "        write_to_file(country_code, country_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    output_dir = \"D:/Master/Sistemi za istrazivanje i analizu podataka/Projekat/YoutubeAPI/Marko/output\"\n",
    "    api_key = \"D:/Master/Sistemi za istrazivanje i analizu podataka/Projekat/YoutubeAPI/Marko/api_key.txt\"\n",
    "    country_codes = \"D:/Master/Sistemi za istrazivanje i analizu podataka/Projekat/YoutubeAPI/Marko/country_codes.txt\"\n",
    "    api_key, country_codes= setup(api_key,country_codes)\n",
    "    get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
